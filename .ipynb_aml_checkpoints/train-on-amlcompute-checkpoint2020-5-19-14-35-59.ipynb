{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Fun using Azure Machine Learning Compute\n",
    "based on [this example](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name[:4], ws.resource_group[:4], ws.location, ws.subscription_id[:10], sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'train-on-amlcompute'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run customer container training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './workspace'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('train.py', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a custom Docker image\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# this is an image available in Docker Hub\n",
    "image_name = 'continuumio/anaconda3'\n",
    "\n",
    "# you can also point to an image in a private ACR\n",
    "image_registry_details = ContainerRegistry()\n",
    "image_registry_details.address = \"myregistry.azurecr.io\"\n",
    "image_registry_details.username = \"username\"\n",
    "image_registry_details.password = \"password\"\n",
    "\n",
    "# Find the compute\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "\n",
    "est = Estimator(source_directory=project_folder, \n",
    "                # We can also pass params:\n",
    "                # script_params={\n",
    "                #     '--numbers-in-sequence': 10\n",
    "                # }, \n",
    "                compute_target= cpu_cluster.name, # We can run even local if docker is present 'local', \n",
    "                entry_script='train.py',\n",
    "                # If I have already all my dependencies baked in the image like in our case\n",
    "                # don't let the system build a new conda environment\n",
    "                user_managed=True,\n",
    "                # Other wise we can define conda dependencies to install which will \n",
    "                # build a custom image on top of the one we specified. E.g. if we\n",
    "                # select miniconda3 which doesn't have any data science packages prebaked\n",
    "                # you needed to uncomment the following:\n",
    "                # user_managed=False, # Optional since this is the default value\n",
    "                # conda_packages=['scikit-learn'],\n",
    "                custom_docker_image=image_name,\n",
    "                # uncomment below line to use your private ACR\n",
    "                # image_registry_details=image_registry_details,\n",
    "                # The following is needed if using default training images \n",
    "                # se_gpu = true,\n",
    "                )\n",
    "\n",
    "est.run_config.save(\"./SampleCustomImage.runconfig\")\n",
    "\n",
    "run = experiment.submit(est)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "# Default datastore, the Azure Blob Store associated with your workspace.\n",
    "def_blob_store = ws.get_default_datastore() \n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))\n",
    "\n",
    "\n",
    "# Upload a file there\n",
    "def_blob_store.upload_files([\"./SampleCustomImage.runconfig\"], target_path=\"SampleUpload\", overwrite=True)\n",
    "print(\"Upload call completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.common.cloudstorageaccount import CloudStorageAccount\n",
    "from azure.storage.common.models import AccessPolicy\n",
    "from azure.storage.blob import BlockBlobService, PageBlobService, AppendBlobService\n",
    "from azure.storage.models import CorsRule, Logging, Metrics, RetentionPolicy, ResourceTypes, AccountPermissions\n",
    "from azure.storage.blob.models import BlobBlock, ContainerPermissions, ContentSettings\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "import json\n",
    "settings= {}\n",
    "with open('./settings.json') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "\n",
    "account_name = settings[\"STORAGE_ACCOUNT_NAME\"]\n",
    "account_key = settings[\"STORAGE_ACCOUNT_KEY\"]\n",
    "\n",
    "account = CloudStorageAccount(account_name, account_key)\n",
    "\n",
    "blobService = account.create_block_blob_service()\n",
    "\n",
    "container_name = \"testattach\"\n",
    "\n",
    "policyId = \"2020-05-01-readlist-access\"\n",
    "\n",
    "# Set access policy on container\n",
    "access_policy = AccessPolicy(permission=ContainerPermissions(read=True, list=True),\n",
    "                                     expiry=datetime.utcnow() + timedelta(hours=10))\n",
    "identifiers = {policyId: access_policy}\n",
    "# Note that all previous policies get removed with the following\n",
    "# If I wanted to maintain them, I should read them and append the new policy\n",
    "acl = blobService.set_container_acl(container_name, identifiers)\n",
    "\n",
    "# Wait 30 seconds for acl to propagate\n",
    "time.sleep(30)\n",
    "\n",
    "\n",
    "# Indicates to use the access policy set on the container\n",
    "token = blobService.generate_container_shared_access_signature(\n",
    "            container_name,\n",
    "            id=policyId\n",
    ")\n",
    "\n",
    "# Avoid storing token in jupyter cell output \n",
    "print(\"Token: {}...\".format(token[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "datastore_name=\"test_datastore\"\n",
    "\n",
    "\n",
    "datastore = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=datastore_name, \n",
    "                      container_name= container_name, \n",
    "                      account_name=account_name, \n",
    "                      sas_token=token,                              \n",
    "                      overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import os\n",
    "\n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "datastore = Datastore(ws, \"test_datastore\")\n",
    "\n",
    "datastore_path = [\n",
    "  DataPath(datastore, '*.txt')\n",
    "]\n",
    "\n",
    "dataset = Dataset.File.from_files(path=datastore_path)\n",
    "dataset_name = 'txt_dataset'\n",
    "\n",
    "# Register the dataset\n",
    "dataset.register(workspace=ws,\n",
    "                 name=dataset_name,\n",
    "                 description='Text files in test_datastore',\n",
    "                 create_new_version=True)\n",
    "\n",
    "# Optionally you can create a temp mounting path\n",
    "# import tempfile\n",
    "# mounted_path = tempfile.mkdtemp()\n",
    "# print (mounted_path)\n",
    "# And you can mount in specific location\n",
    "# with dataset.mount(mounted_path) as mount_context:\n",
    "\n",
    "with dataset.mount() as mount_context:\n",
    "    mount_context.start()\n",
    "    # This is the point where the sataset is mounted\n",
    "    print(mount_context.mount_point)\n",
    "    # list top level mounted files and folders in the dataset\n",
    "    print(os.listdir(mount_context.mount_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's attach the dataset to the estimator\n",
    "# First let's show how to get the dataset by name\n",
    "dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "print(f\"The id of the dataset is {dataset.id}\")\n",
    "\n",
    "script_params = {\n",
    "    # mount the dataset on the remote compute and pass the mounted path as an argument to the training script\n",
    "    # the as_named_input also exposes the mount point as the DATA_FOLDER environment variable\n",
    "    # It will also be accessible via run.input_datasets['DATA_FOLDER'] if you reference the azureML SDK\n",
    "    # https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.abstract_dataset.abstractdataset?view=azure-ml-py#as-named-input-name-\n",
    "    '--data-folder': dataset.as_named_input('DATA_FOLDER').as_mount()\n",
    "}\n",
    "\n",
    "# Default conda image is with debian thus we will use stock image.\n",
    "# Dataset initialization failed: Unsupported Linux distribution debian 10.\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-custom-docker-image#create-a-custom-base-image\n",
    "\n",
    "est = Estimator(source_directory=project_folder, \n",
    "                # Pass the param to make the script ls the dir\n",
    "                script_params = script_params, \n",
    "                compute_target= cpu_cluster.name, # We can run even local if docker is present 'local', \n",
    "                entry_script='train.py',\n",
    "                # We need to install azureml to handle the mounting of the dataset\n",
    "                user_managed=False,\n",
    "                conda_packages=['scikit-learn'],\n",
    "                pip_packages=['azureml-dataprep[fuse,pandas]'],\n",
    "                )\n",
    "\n",
    "# The following will fail to serialize\n",
    "# est.run_config.save(\"./WithMount_CustomImage.runconfig\")\n",
    "# But you can see the json file submitted to AzureML and then construct the run config as in the following sample\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-train-deploy-model-cli#reference-the-dataset\n",
    "\n",
    "# The arguments to the script file.\n",
    "# arguments:\n",
    "# - --data-folder\n",
    "# - DatasetConsumptionConfig:DATA_FOLDER\n",
    "# .....\n",
    "# The configuration details for data.\n",
    "#data:\n",
    "#  DATA_FOLDER:\n",
    "## Data Location\n",
    "#    dataLocation:\n",
    "## the Dataset used for this run.\n",
    "#      dataset:\n",
    "## Id of the dataset.\n",
    "#        id: a3fc9be9-075c-40b4-b23a-45befff4df50\n",
    "## the DataPath used for this run.\n",
    "#      datapath:\n",
    "## Whether to create new folder.\n",
    "#    createOutputDirectories: false\n",
    "## The mode to handle\n",
    "#    mechanism: mount\n",
    "## Point where the data is download or mount or upload.\n",
    "#    environmentVariableName: DATA_FOLDER\n",
    "## relative path where the data is download or mount or upload.\n",
    "#    pathOnCompute:\n",
    "## Whether to overwrite the data if existing.\n",
    "#    overwrite: false\n",
    "\n",
    "run = experiment.submit(est)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLOB CONTAINER LEASE STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  RESET LEASE STATUS OF A CONTAINER ###########\n",
    "from azure.storage.common.cloudstorageaccount import CloudStorageAccount\n",
    "\n",
    "# Retrieve the storage account and the storage key\n",
    "import json\n",
    "settings= {}\n",
    "with open('./settings.json') as f:\n",
    "    settings = json.load(f)\n",
    "account_name = settings[\"STORAGE_ACCOUNT_NAME\"]\n",
    "account_key = settings[\"STORAGE_ACCOUNT_KEY\"]\n",
    "\n",
    "# The container that has Lease status broken\n",
    "container_name='test-lease'\n",
    "\n",
    "# Create a blobservice client from a storage account client\n",
    "account = CloudStorageAccount(account_name, account_key)\n",
    "blobService = account.create_block_blob_service()\n",
    "    \n",
    "# Get a container lease\n",
    "lease_id=blobService.acquire_container_lease(container_name, lease_duration=-1)\n",
    "# Release that lease\n",
    "blobService.release_container_lease(container_name,lease_id)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "rndazurescript"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "training"
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Diabetes"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "None"
  ],
  "friendly_name": "Train on Azure Machine Learning Compute",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "tags": [
   "None"
  ],
  "task": "Submit a run on Azure Machine Learning Compute."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
