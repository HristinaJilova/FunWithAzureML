{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create a simple/non parallel batch inference pipeline\r\n",
        "\r\n",
        "Create a batch inference pipeline that reads data from the registered dataset and store the outputs in the default datastore in the path defined in the variables cell bellow."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables used in script\r\n",
        "model_name = \"simple-model\" # This is also used in the requirements notebook to register the model\r\n",
        "dataset_name = \"simple-batch\" # This is also used in the requirements notebook to register the dataset\r\n",
        "\r\n",
        "compute_cluster_name='cpu-cluster'\r\n",
        "pipeline_name='simple-batch-pipeline'\r\n",
        "output_folder = \"simple-batch-output\" # folder in default store to output csv file"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614613795679
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the inference script\r\n",
        "Generate the python script that will be doing the batch inference:\r\n",
        "- Load the model by name. Model name passed as parameter.\r\n",
        "- Load dataset. Name of dataset passed as parameter. The script could be loading the inputs from the datastore directly instead of having to register a dataset.\r\n",
        "- Make inferences\r\n",
        "- Store output in a folder passed as an argument. Script doesn't need to know what the output is going to be\r\n",
        "\r\n",
        "\r\n",
        "We are also copying the SimpleModel module next to the script and generate a conda environment which will be used by the pipeline.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder to store the script\r\n",
        "import os\r\n",
        "\r\n",
        "script_path = 'script_folder'\r\n",
        "\r\n",
        "if not os.path.exists(script_path):\r\n",
        "    os.makedirs(script_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614612262445
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the module that contains our simple model\r\n",
        "# This is a dependency we need to bake with the scoring file\r\n",
        "! cp SimpleModel.py script_folder/"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614612262704
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile script_folder/inference.py\r\n",
        "\r\n",
        "import argparse\r\n",
        "\r\n",
        "# retrieve arguments of this script\r\n",
        "# Default values added so that we can run locally the script and debug\r\n",
        "# python inference.py\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\r\n",
        "    \"--input-dataset\",\r\n",
        "    type=str,\r\n",
        "    dest=\"input_dataset_name\",\r\n",
        "    help=\"The dataset name to use for inference\",\r\n",
        "    default=\"simple-batch\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--model-name\",\r\n",
        "    type=str,\r\n",
        "    dest=\"model_name\",\r\n",
        "    help=\"The model to use to do inferences\",\r\n",
        "    default=\"simple-model\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--output-folder\",\r\n",
        "    type=str,\r\n",
        "    dest=\"output_folder\",\r\n",
        "    help=\"Where to store the processed outputs in csv files\",\r\n",
        "    default=\"./output\",\r\n",
        ")\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "# Get a reference to the workspace to be able to load things\r\n",
        "from azureml.core import Workspace\r\n",
        "from azureml.core.run import Run, _OfflineRun\r\n",
        "\r\n",
        "run = Run.get_context()\r\n",
        "ws = None\r\n",
        "if type(run) == _OfflineRun:\r\n",
        "    ws = Workspace.from_config()\r\n",
        "else:\r\n",
        "    ws = run.experiment.workspace\r\n",
        "\r\n",
        "# Download and dehydrate the model\r\n",
        "from azureml.core import Model\r\n",
        "\r\n",
        "azureml_model = Model(ws, args.model_name)\r\n",
        "# Download latest artifacts, overriding them\r\n",
        "azureml_model.download(\"./\", exist_ok=True)\r\n",
        "\r\n",
        "import os, joblib\r\n",
        "from SimpleModel import SimpleModel\r\n",
        "\r\n",
        "model_path = os.path.join(\"model\", \"model.joblib\")\r\n",
        "model = joblib.load(model_path)  # Note that we don't call the constructor\r\n",
        "# The target column is stored in the instance we hydrated in the\r\n",
        "# requirements notebook\r\n",
        "\r\n",
        "# Get dataset\r\n",
        "from azureml.core import Dataset\r\n",
        "\r\n",
        "ds = Dataset.get_by_name(ws, args.input_dataset_name)\r\n",
        "inferenced_df = ds.to_pandas_dataframe()\r\n",
        "\r\n",
        "# Make inferences and store them as a new column in the dataset\r\n",
        "inferenced_df[\"outputs\"] = model.predict(inferenced_df)\r\n",
        "\r\n",
        "# Store the results\r\n",
        "# Create output path if not exists\r\n",
        "import os\r\n",
        "\r\n",
        "output_path = args.output_folder\r\n",
        "if not os.path.exists(output_path):\r\n",
        "    os.makedirs(output_path)\r\n",
        "output_file_path = os.path.join(output_path, \"results.csv\")\r\n",
        "inferenced_df.to_csv(output_file_path, index=False)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile script_folder/conda_env.yaml\r\n",
        "# Packages should have explicit versions \r\n",
        "# For demo purposes we let them loose\r\n",
        "# Also note the azureml-defaults package mentioned in  https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments#types-of-environments\r\n",
        "name: inference-env\r\n",
        "dependencies:\r\n",
        "  - python=3.6\r\n",
        "  - scikit-learn\r\n",
        "  - pip\r\n",
        "  - pip:\r\n",
        "    - azureml-defaults"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core.compute import ComputeTarget\r\n",
        "\r\n",
        "# Connect to workspace and get resource references\r\n",
        "ws = Workspace.from_config()\r\n",
        "compute_cluster = ComputeTarget(workspace=ws, name=compute_cluster_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614615540303
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "\r\n",
        "# Create an environment from the conda dependencies\r\n",
        "run_env = Environment.from_conda_specification(\"run-environment\",  'script_folder/conda_env.yaml')\r\n",
        "# Create a run config that we will use in our steps\r\n",
        "run_config = RunConfiguration()\r\n",
        "run_config.environment = run_env\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614615543604
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "# To find more help\r\n",
        "# help(OutputFileDatasetConfig)\r\n",
        "datastore = ws.get_default_datastore()\r\n",
        "\r\n",
        "# Create an output folder\r\n",
        "output = OutputFileDatasetConfig(destination=(datastore, output_folder))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614615545718
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "\r\n",
        "# First step to run the inference script\r\n",
        "step_01 = PythonScriptStep(\r\n",
        "    'inference.py',\r\n",
        "    source_directory='script_folder',\r\n",
        "    name='Run inference',\r\n",
        "    compute_target=compute_cluster,\r\n",
        "    runconfig=run_config,\r\n",
        "    allow_reuse= False,\r\n",
        "    arguments=[\r\n",
        "        \"--input-dataset\", dataset_name,\r\n",
        "        \"--model-name\", model_name,\r\n",
        "        \"--output-folder\", output\r\n",
        "    ],\r\n",
        "    outputs=[output]\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614615548705
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import Pipeline\r\n",
        "\r\n",
        "pipeline = Pipeline(workspace=ws, steps=[step_01])\r\n",
        "\r\n",
        "published_pipeline = pipeline.publish(\r\n",
        "    pipeline_name, \r\n",
        "    description=\"Batch inference with predefined output\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614615556859
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "\r\n",
        "# Submit the pipeline to be run\r\n",
        "pipeline_run1 = Experiment(ws, f\"{pipeline_name}-runs\").submit(published_pipeline)\r\n",
        "pipeline_run1.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}