{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Create a training pipeline\r\n",
    "\r\n",
    "Assuming you have a compute cluster named `cpu-cluster`, this notebook will create a pipeline that will be training the model, accepting as a parameter the target country.\r\n",
    "Intermediate results are stored in the default datastore.\r\n",
    "\r\n",
    "The pipeline will have the following parameters:\r\n",
    "- country: The target country to train the model\r\n",
    "- target-day: The reference day to load the data from\r\n",
    "\r\n",
    "The pipeline will have 3 steps:\r\n",
    "- `step_00_prepare_data.py`: This file loads the country specific data and transforms them into lightgbm Dataset.\r\n",
    "- `step_01_train_model.py`: Trains the lightgbm and stores the model and the feature importance into the output folder.\r\n",
    "- `step_02_register_model.py`: Registers all artifacts stored by the previous step in the Model registry. Note that within the model artifacts we store both the joblib model and the feature importance.\r\n",
    "\r\n",
    "Note: Normally the scripts should have been in separate folders in order to upload only the needed scripts for each step. In this example, we upload all of them. This means that every time we change a single step, the [whole folder is snapshotted](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328359945
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Variables used in script\r\n",
    "compute_cluster_name='cpu-cluster'\r\n",
    "pipeline_name='train-churn-model-pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328360271
    }
   },
   "outputs": [],
   "source": [
    "# [Optionally] upgrade to latest AzureML SDK. If the next cell throws errors,\r\n",
    "# you may need to restart the compute\r\n",
    "# !pip install --upgrade azureml-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328368654
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import azureml\r\n",
    "\r\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\r\n",
    "from azureml.data.data_reference import DataReference\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\r\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.train.estimator import Estimator\r\n",
    "import os\r\n",
    "\r\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328371106
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Connect to workspace and get resource references\r\n",
    "ws = Workspace.from_config()\r\n",
    "compute_cluster = ComputeTarget(workspace=ws, name=compute_cluster_name)\r\n",
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328371606
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define the pipeline parameters \r\n",
    "country_pipeline_param = PipelineParameter(name=\"country\", default_value=\"GR\")\r\n",
    "target_day_param = PipelineParameter(name=\"target-day\", default_value=\"2020-11-26\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Python environment\r\n",
    "We are going to create a single python environment where we will run all steps.\r\n",
    "Ideally, I could have separate requirements.txt file, one per step (e.g. instead of training_environment I would have step_00_environment, step_01_environment etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328372041
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create an environment with the pip requirements\r\n",
    "training_environment = Environment.from_pip_requirements('training_environment', 'training_pipeline_requirements.txt')\r\n",
    "# Create a run config that we will use in our steps\r\n",
    "training_run_config = RunConfiguration()\r\n",
    "training_run_config.environment = training_environment\r\n",
    "# For more samples about environments have a look at\r\n",
    "# https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/using-environments/using-environments.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 00 Data preparation\r\n",
    "The script accepts three arguments:\r\n",
    "- country: This is a pipeline parameter\r\n",
    "- target-day: The date partition from where we are going to read the training dataset\r\n",
    "- output-path: Where to store the processed dataset\r\n",
    "\r\n",
    "With in the script, the output-path is just a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328372457
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# We are going to store the prepared data as an intermediate step\r\n",
    "training_data_path = PipelineData(\r\n",
    "    \"training_data\",\r\n",
    "    datastore=datastore,\r\n",
    "    is_directory=True\r\n",
    ")\r\n",
    "\r\n",
    "# This is the first step\r\n",
    "step_00 = PythonScriptStep(\r\n",
    "    'step_00_prepare_data.py',\r\n",
    "    source_directory='.',\r\n",
    "    name='Prepare data',\r\n",
    "    compute_target=compute_cluster,\r\n",
    "    arguments=[\r\n",
    "        \"--country\", country_pipeline_param, \r\n",
    "        \"--target-day\", target_day_param,\r\n",
    "        \"--output-path\", training_data_path\r\n",
    "        ],\r\n",
    "    runconfig=training_run_config,\r\n",
    "    outputs=[training_data_path],\r\n",
    "    allow_reuse=True # Allow reuse of the data prep step\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 01 Training\r\n",
    "The script has the following inputs:\r\n",
    "- input-path: A folder that contains the processed dataset\r\n",
    "- output-path: A folder to store the model and the feature importance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328372857
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# We are going to store the prepared data as an intermediate step\r\n",
    "model_store_path = PipelineData(\r\n",
    "    \"model_store\",\r\n",
    "    datastore=datastore,\r\n",
    "    is_directory=True\r\n",
    ")\r\n",
    "\r\n",
    "# This is the first step\r\n",
    "step_01 = PythonScriptStep(\r\n",
    "    'step_01_train_model.py',\r\n",
    "    source_directory='.',\r\n",
    "    name='Train churn model',\r\n",
    "    compute_target=compute_cluster,\r\n",
    "    arguments=[\r\n",
    "        \"--input-path\", training_data_path, \r\n",
    "        \"--output-path\", model_store_path\r\n",
    "        ],\r\n",
    "    runconfig=training_run_config,\r\n",
    "    inputs=[training_data_path],\r\n",
    "    outputs=[model_store_path],\r\n",
    "    allow_reuse=True # Allow reuse of the data prep step\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 02 Register model\r\n",
    "This script has the following arguments:\r\n",
    "- input-path: The folder that contains the model and the feature importance dataset\r\n",
    "- country: The pipeline parameter passed in Step 00. This is used to append in the registered model name.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328373247
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# This is the first step\r\n",
    "step_02 = PythonScriptStep(\r\n",
    "    'step_02_register_model.py',\r\n",
    "    source_directory='.',\r\n",
    "    name='Register churn model',\r\n",
    "    compute_target=compute_cluster,\r\n",
    "    arguments=[\r\n",
    "        \"--input-path\", model_store_path, \r\n",
    "        \"--country\", country_pipeline_param, \r\n",
    "    ],\r\n",
    "    runconfig=training_run_config,\r\n",
    "    inputs=[model_store_path],\r\n",
    "    allow_reuse=True # Allow reuse of the data prep step\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create the pipeline\r\n",
    "Now that we have all steps ready, we create and publish a pipeline that we can be invoking through Azure Data Factory, simple REST API call or even schedule it's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328373843
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "\n",
    "# Disable previously registered pipelines\n",
    "published_pipeline = PublishedPipeline.list(workspace=ws)\n",
    "for x in published_pipeline:\n",
    "    if (x.name == pipeline_name):\n",
    "        x.disable()\n",
    "        print(\"Disabled pipeline with id \", x.id, \" and name \", x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607328410027
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[step_00, step_01, step_02])\r\n",
    "\r\n",
    "published_pipeline = pipeline.publish(\r\n",
    "    pipeline_name, \r\n",
    "    description=\"Train a churn model for a specific country\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
